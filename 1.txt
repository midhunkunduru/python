config

from dataclasses import dataclass
import os

@dataclass
class AzureSQLConfig:
    server: str = os.getenv("AZSQL_SERVER", "")
    database: str = os.getenv("AZSQL_DATABASE", "")
    auth_mode: str = os.getenv("AZSQL_AUTH_MODE", "msi").lower()  # msi|token|interactive|password|spn
    username: str | None = os.getenv("AZSQL_USERNAME")
    password: str | None = os.getenv("AZSQL_PASSWORD")
    client_id: str | None = os.getenv("AZURE_CLIENT_ID")
    tenant_id: str | None = os.getenv("AZURE_TENANT_ID")
    client_secret: str | None = os.getenv("AZURE_CLIENT_SECRET")
    driver: str = os.getenv("AZSQL_DRIVER", "{ODBC Driver 18 for SQL Server}")
    encrypt: str = os.getenv("AZSQL_ENCRYPT", "yes")
    trust: str = os.getenv("AZSQL_TRUST", "no")
    timeout: str = os.getenv("AZSQL_CONN_TIMEOUT", "30")

@dataclass
class SplunkConfig:
    host: str = os.getenv("SPLUNK_HOST", "")
    port: int = int(os.getenv("SPLUNK_PORT", "8089"))
    scheme: str = os.getenv("SPLUNK_SCHEME", "https")
    username: str | None = os.getenv("SPLUNK_USERNAME")
    password: str | None = os.getenv("SPLUNK_PASSWORD")
    query: str = os.getenv("SPLUNK_QUERY", 'search index="test_developer_dev" TriggerBy="SprintChanged"')
    earliest_time: str = os.getenv("SPLUNK_EARLIEST", "-24h")
    latest_time: str = os.getenv("SPLUNK_LATEST", "now")

@dataclass
class TableNames:
    jira_sprint: str = os.getenv("TBL_JIRA_SPRINT", "dbo.jira_sprint")
    jira_issue:  str = os.getenv("TBL_JIRA_ISSUE", "dbo.jira_issue")
    jira_assign: str = os.getenv("TBL_JIRA_ASSIGN", "dbo.jira_sprint_issue_assigned")


dbsource

from __future__ import annotations
import datetime as dt
from typing import Any, Dict, List, Tuple, Optional

class TypeUtils:
    @staticmethod
    def parse_dt(value: Any) -> Optional[dt.datetime]:
        if value is None: return None
        if isinstance(value, dt.datetime):
            return value.astimezone(dt.timezone.utc).replace(tzinfo=None) if value.tzinfo else value
        s = str(value).strip()
        if not s: return None
        try:
            return dt.datetime.fromisoformat(s.replace("Z", "+00:00")).astimezone(dt.timezone.utc).replace(tzinfo=None)
        except Exception:
            pass
        for fmt in ("%a %b %d %H:%M:%S %Z %Y", "%a %b %d %H:%M:%S %Y", "%Y-%m-%d %H:%M:%S"):
            try:
                d = dt.datetime.strptime(s, fmt)
                return d if d.tzinfo is None else d.astimezone(dt.timezone.utc).replace(tzinfo=None)
            except Exception:
                continue
        try:
            return dt.datetime.fromtimestamp(float(s), tz=dt.timezone.utc).replace(tzinfo=None)
        except Exception:
            return None

    @staticmethod
    def parse_bool(v: Any) -> Optional[bool]:
        if v is None: return None
        s = str(v).strip().lower()
        if s in ("true","t","1","yes","y"): return True
        if s in ("false","f","0","no","n"): return False
        return None

    @staticmethod
    def parse_int(v: Any) -> Optional[int]:
        if v is None or str(v).strip()=="":
            return None
        try: return int(v)
        except Exception: return None

    @staticmethod
    def parse_float(v: Any) -> Optional[float]:
        if v is None or str(v).strip()=="":
            return None
        try: return float(v)
        except Exception: return None

    @staticmethod
    def coerce_value(sql_type: str, val: Any) -> Any:
        if val is None: return None
        t = (sql_type or "").lower()
        if any(k in t for k in ("char","text","nchar","ntext","varchar","nvarchar")):
            return str(val)
        if any(k in t for k in ("date","time")):
            return val if isinstance(val, dt.datetime) else TypeUtils.parse_dt(val)
        if "int" in t:
            return val if isinstance(val, int) else TypeUtils.parse_int(val)
        if any(k in t for k in ("decimal","numeric","float","real","money")):
            return val if isinstance(val, (int,float)) else TypeUtils.parse_float(val)
        if "bit" in t or "bool" in t:
            return val if isinstance(val, bool) else TypeUtils.parse_bool(val)
        return val

    @staticmethod
    def coerce_row_to_schema(row: Dict[str, Any], schema: List[Tuple[str,str]]) -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        for col, sql_type in schema:
            out[col] = TypeUtils.coerce_value(sql_type, row.get(col))
        return out




splunkdata

from typing import List, Dict, Any
import logging
from splunklib import client, results
from .confmanager import SplunkConfig

logger = logging.getLogger(__name__)

class SplunkSource:
    def __init__(self, cfg: SplunkConfig | None = None):
        self.cfg = cfg or SplunkConfig()
        self.service = None

    def connect(self):
        if self.service: return self.service
        logger.info("Connecting Splunk %s:%s", self.cfg.host, self.cfg.port)
        self.service = client.connect(
            host=self.cfg.host, port=self.cfg.port, scheme=self.cfg.scheme,
            username=self.cfg.username, password=self.cfg.password
        )
        return self.service

    def read_jira_sprint_events(self, query: str | None = None) -> List[Dict[str, Any]]:
        svc = self.connect()
        q = query or self.cfg.query
        logger.info("Splunk query: %s", q)
        job = svc.jobs.create(q, exec_mode="blocking",
                              earliest_time=self.cfg.earliest_time,
                              latest_time=self.cfg.latest_time)
        resp = job.results(output_mode="json", count=0)
        reader = results.JSONResultsReader(resp)
        rows: List[Dict[str, Any]] = []
        for item in reader:
            if isinstance(item, dict):
                rows.append(item)
        logger.info("Splunk returned %d rows", len(rows))
        return rows




transform


from __future__ import annotations
import json
import logging
from typing import Any, Dict, List, Optional
from .dbsource import TypeUtils

logger = logging.getLogger(__name__)

def _to_list(v: Any) -> List[str]:
    if v is None: return []
    if isinstance(v, (list,tuple)):
        return [str(x).strip() for x in v if str(x).strip()]
    return [p.strip() for p in str(v).split(",") if p.strip()]

class Transformer:
    @staticmethod
    def parse_raw_json(row: Dict[str, Any]) -> Dict[str, Any]:
        if isinstance(row, dict) and "_raw" in row:
            try: return json.loads(row["_raw"])
            except Exception:
                logger.warning("Failed _raw JSON parse")
                return {}
        return {}

    def explode_sprints(self, payload: Dict[str, Any], meta: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        project = (payload.get("project") or {}).get("name")
        s = (payload.get("sprint") or {})
        names = _to_list(s.get("name")) or [""]
        event_dt = TypeUtils.parse_dt((meta or {}).get("_time")) if meta else None
        rows: List[Dict[str, Any]] = []
        for sn in names:
            rows.append({
                "ProjectName": project,
                "SprintName": sn,
                "SprintBoardId": s.get("originBoardId"),
                "SprintStartDate": TypeUtils.parse_dt(s.get("startDate")),
                "SprintEndDate": TypeUtils.parse_dt(s.get("endDate")),
                "SprintIsStarted": s.get("isStarted"),
                "SprintIsClosed": s.get("isClosed"),
                "SprintCompletedDate": TypeUtils.parse_dt(s.get("completeDate")),
                "SprintGoal": s.get("goal"),
                "EventDate": event_dt,
            })
        return rows

    def build_issue_row(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        issue = payload.get("issue") or {}
        project = payload.get("project") or {}
        sp = issue.get("Story Points") or issue.get("storyPoints")
        try:
            sp = float(sp) if sp not in (None,"","null") else None
        except Exception:
            sp = None
        return {
            "Issue": issue.get("key"),
            "ProjectName": project.get("name"),
            "IssueSummary": issue.get("summary"),
            "IssueTypeName": (issue.get("issueType") or {}).get("name"),
            "IssueAssigneeName": (issue.get("assignee") or {}).get("displayName"),
            "IssueParentKey": (issue.get("parent") or {}).get("key"),
            "IssueParentSummary": (issue.get("parent") or {}).get("summary"),
            "IssueStatusName": (issue.get("status") or {}).get("name"),
            "IssueStoryPoints": sp,
        }

    def explode_assignments_from_meta_time(self, payload: Dict[str, Any], meta: Dict[str, Any]) -> List[Dict[str, Any]]:
        issue   = (payload.get("issue")   or {}).get("key")
        project = (payload.get("project") or {}).get("name")
        sprints = _to_list((payload.get("sprint") or {}).get("name")) or [""]
        event_dt = TypeUtils.parse_dt(meta.get("_time"))
        rows: List[Dict[str, Any]] = []
        for sn in sprints:
            rows.append({"Issue": issue, "ProjectName": project, "SprintName": sn, "EventDate": event_dt})
        # per-record dedupe
        seen, uniq = set(), []
        for r in rows:
            k = (r["Issue"], r["ProjectName"], r["SprintName"], r["EventDate"])
            if k in seen: continue
            seen.add(k); uniq.append(r)
        return uniq


rules


from dataclasses import dataclass
from typing import List
from .jirasprintchangemgr import FieldRule, to_trim, to_dt, to_bool, to_float, is_datetime, is_alnum, max_len

@dataclass
class SprintRules:
    def get_rules(self) -> List[FieldRule]:
        return [
            FieldRule("ProjectName", required=True,  normalize=to_trim, check=lambda v: is_alnum(v) and max_len(400)(v)),
            FieldRule("SprintName",  required=True,  normalize=to_trim, check=lambda v: is_alnum(v) and max_len(400)(v)),
            FieldRule("SprintBoardId", required=False, normalize=to_trim, check=max_len(200)),
            FieldRule("SprintStartDate", required=False, normalize=to_dt, check=is_datetime),
            FieldRule("SprintEndDate",   required=False, normalize=to_dt, check=is_datetime),
            FieldRule("SprintCompletedDate", required=False, normalize=to_dt, check=is_datetime),
            FieldRule("SprintIsStarted", required=False, normalize=to_bool),
            FieldRule("SprintIsClosed",  required=False, normalize=to_bool),
            FieldRule("SprintGoal",      required=False, normalize=to_trim, check=max_len(4000)),
            FieldRule("EventDate",       required=False, normalize=to_dt, check=is_datetime),
        ]
    @staticmethod
    def crosscheck(row):
        errs = {}
        sd, ed, cd = row.get("SprintStartDate"), row.get("SprintEndDate"), row.get("SprintCompletedDate")
        if sd and ed and sd > ed: errs["SprintStartDate"] = "SprintStartDate <= SprintEndDate"
        if ed and cd and ed > cd: errs["SprintCompletedDate"] = "SprintCompletedDate >= SprintEndDate"
        return errs

@dataclass
class IssueRules:
    def get_rules(self) -> List[FieldRule]:
        return [
            FieldRule("Issue", required=True, normalize=to_trim, check=lambda v: is_alnum(v) and max_len(200)(v)),
            FieldRule("ProjectName", required=False, normalize=to_trim, check=max_len(400)),
            FieldRule("IssueSummary", required=False, normalize=to_trim, check=max_len(4000)),
            FieldRule("IssueTypeName", required=False, normalize=to_trim, check=max_len(200)),
            FieldRule("IssueAssigneeName", required=False, normalize=to_trim, check=max_len(400)),
            FieldRule("IssueParentKey", required=False, normalize=to_trim, check=max_len(200)),
            FieldRule("IssueParentSummary", required=False, normalize=to_trim, check=max_len(4000)),
            FieldRule("IssueStatusName", required=False, normalize=to_trim, check=max_len(200)),
            FieldRule("IssueStoryPoints", required=False, normalize=to_float),
        ]

@dataclass
class AssignRules:
    def get_rules(self) -> List[FieldRule]:
        return [
            FieldRule("Issue",       required=True,  normalize=to_trim, check=lambda v: is_alnum(v) and max_len(200)(v)),
            FieldRule("ProjectName", required=True,  normalize=to_trim, check=lambda v: is_alnum(v) and max_len(400)(v)),
            FieldRule("SprintName",  required=True,  normalize=to_trim, check=lambda v: is_alnum(v) and max_len(400)(v)),
            FieldRule("EventDate",   required=True,  normalize=to_dt,   check=is_datetime),
        ]



changemgr

from __future__ import annotations
import json, os, logging, re
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Tuple
from .dbsource import TypeUtils
from .loader import Database

logger = logging.getLogger(__name__)

# ----- Field rules -----
@dataclass
class FieldRule:
    name: str
    required: bool = False
    normalize: Optional[Callable[[Any], Any]] = None
    check: Optional[Callable[[Any], bool]] = None
    message: Optional[str] = None

_ALNUM = re.compile(r"^[A-Za-z0-9 ._\-:/()\[\],]+$")

def to_trim(v): return None if v is None else str(v).strip()
def to_dt(v):  d = TypeUtils.parse_dt(v); return d if d is not None else v
def to_bool(v): b = TypeUtils.parse_bool(v); return b if b is not None else v
def to_float(v): f = TypeUtils.parse_float(v); return f if f is not None else v
def is_not_null(v): return v is not None and str(v).strip() != ""
def is_alnum(v): return bool(_ALNUM.match(str(v).strip())) if v is not None else False
def max_len(n): return lambda v: (v is None) or len(str(v)) <= n
def is_datetime(v): return TypeUtils.parse_dt(v) is not None

# ----- Row/Entity validators -----
@dataclass
class ValidationResult:
    ok: bool
    errors: Dict[str, str]
    row: Dict[str, Any]

class RowValidator:
    def __init__(self, rules: List[FieldRule]): self.rules = rules
    def validate(self, row: Dict[str, Any]) -> ValidationResult:
        out, errs = dict(row), {}
        for r in self.rules:
            val = out.get(r.name)
            if r.normalize:
                try: val = r.normalize(val); out[r.name] = val
                except Exception: errs[r.name] = r.message or f"{r.name} normalization failed"; continue
            if r.required and not is_not_null(val):
                errs[r.name] = r.message or f"{r.name} is required"; continue
            if r.check and is_not_null(val):
                try: ok = r.check(val)
                except Exception: ok = False
                if not ok: errs[r.name] = r.message or f"{r.name} failed validation"
        return ValidationResult(ok=(len(errs)==0), errors=errs, row=out)

class EntityValidator:
    def __init__(self, rules: List[FieldRule], crosscheck: Optional[Callable[[Dict[str, Any]], Dict[str, str]]] = None):
        self.row_validator = RowValidator(rules)
        self.crosscheck = crosscheck or (lambda _row: {})
    def validate_rows(self, rows: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        good, bad = [], []
        for r in rows:
            res = self.row_validator.validate(r)
            if res.ok:
                xerrs = self.crosscheck(res.row)
                if xerrs: bad.append({"why": xerrs, "row": r})
                else:     good.append(res.row)
            else:
                bad.append({"why": res.errors, "row": r})
        return good, bad

# ----- Pipeline -----
class DataQualityPipeline:
    def __init__(self, db: Database,
                 sprint_rules: List[FieldRule], issue_rules: List[FieldRule], assign_rules: List[FieldRule],
                 sprint_cross=None, issue_cross=None, assign_cross=None,
                 quarantine_dir: str = "./quarantine", logger_: Optional[logging.Logger]=None):
        self.db = db; self.quarantine_dir = quarantine_dir
        self.logger = logger_ or logging.getLogger(__name__)
        os.makedirs(self.quarantine_dir, exist_ok=True)
        self.sprint_val = EntityValidator(sprint_rules, sprint_cross or (lambda r:{}))
        self.issue_val  = EntityValidator(issue_rules,  issue_cross  or (lambda r:{}))
        self.assign_val = EntityValidator(assign_rules, assign_cross or (lambda r:{}))

    def _qwrite(self, name: str, records: List[Dict[str, Any]]):
        if not records: return
        path = os.path.join(self.quarantine_dir, name)
        with open(path, "a", encoding="utf-8") as f:
            for rec in records: f.write(json.dumps(rec, default=str) + "\n")

    def run_and_load(self, sprint_rows, issue_rows, assign_rows):
        gs, bs = self.sprint_val.validate_rows(sprint_rows)
        gi, bi = self.issue_val.validate_rows(issue_rows)
        ga, ba = self.assign_val.validate_rows(assign_rows)
        self._qwrite("invalid_sprints.jsonl", bs)
        self._qwrite("invalid_issues.jsonl", bi)
        self._qwrite("invalid_assignments.jsonl", ba)
        self.logger.info("Validation: sprints ok=%d bad=%d | issues ok=%d bad=%d | assignments ok=%d bad=%d",
                         len(gs), len(bs), len(gi), len(bi), len(ga), len(ba))

        # Dynamic coercion by DB schema
        gs = self.db.coerce_rows_to_table_schema(self.db.tables.jira_sprint, gs)
        gi = self.db.coerce_rows_to_table_schema(self.db.tables.jira_issue,  gi)
        ga = self.db.coerce_rows_to_table_schema(self.db.tables.jira_assign, ga)

        # Upserts with natural keys
        self.db.merge_rows(self.db.tables.jira_sprint, gs, key_cols=("ProjectName","SprintName"))
        self.db.merge_rows(self.db.tables.jira_issue,  gi, key_cols=("Issue",))
        self.db.merge_rows(self.db.tables.jira_assign, ga, key_cols=("Issue","ProjectName","SprintName"))

        return {"sprint_ok":len(gs),"sprint_bad":len(bs),
                "issue_ok":len(gi),"issue_bad":len(bi),
                "assign_ok":len(ga),"assign_bad":len(ba)}




                loader

                from __future__ import annotations
import logging
from typing import Any, Dict, List, Tuple, Optional
import pyodbc
from .confmanager import AzureSQLConfig, TableNames
from .dbsource import TypeUtils

logger = logging.getLogger(__name__)
SQL_COPT_SS_ACCESS_TOKEN = 1256

class Database:
    def __init__(self, cfg: AzureSQLConfig | None = None, tables: TableNames | None = None):
        self.cfg = cfg or AzureSQLConfig()
        self.tables = tables or TableNames()
        self.cn: Optional[pyodbc.Connection] = None
        self._schema_cache: Dict[str, List[Tuple[str,str,bool]]] = {}
        self._id_cache: Dict[str, set[str]] = {}

    # ---- connection ----
    def _base_conn_str(self) -> str:
        return (
            f"DRIVER={self.cfg.driver};"
            f"SERVER=tcp:{self.cfg.server},1433;"
            f"DATABASE={self.cfg.database};"
            f"Encrypt={self.cfg.encrypt};"
            f"TrustServerCertificate={self.cfg.trust};"
            f"Connection Timeout={self.cfg.timeout};"
        )

    def connect(self) -> pyodbc.Connection:
        if self.cn: return self.cn
        mode = (self.cfg.auth_mode or "msi").lower()
        logger.info("SQL connect mode=%s server=%s db=%s", mode, self.cfg.server, self.cfg.database)
        if mode == "msi":
            cs = self._base_conn_str() + "Authentication=ActiveDirectoryMsi;"
            if self.cfg.client_id:
                cs += f"Client Id={self.cfg.client_id};"
            self.cn = pyodbc.connect(cs)
        elif mode == "token":
            from azure.identity import DefaultAzureCredential
            cred = DefaultAzureCredential(managed_identity_client_id=self.cfg.client_id)
            token = cred.get_token("https://database.windows.net/.default").token
            token_bytes = token.encode("utf-16le")
            self.cn = pyodbc.connect(self._base_conn_str(), attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_bytes})
        elif mode == "interactive":
            self.cn = pyodbc.connect(self._base_conn_str() + "Authentication=ActiveDirectoryInteractive;")
        elif mode == "password":
            cs = self._base_conn_str() + f"Authentication=ActiveDirectoryPassword;UID={self.cfg.username};PWD={self.cfg.password};"
            self.cn = pyodbc.connect(cs)
        elif mode == "spn":
            cs = (self._base_conn_str()
                  + "Authentication=ActiveDirectoryServicePrincipal;"
                  + f"Application Id={self.cfg.client_id};"
                  + f"Client Secret={self.cfg.client_secret};"
                  + f"Tenant Id={self.cfg.tenant_id};")
            self.cn = pyodbc.connect(cs)
        else:
            raise ValueError(f"Unknown auth mode: {mode}")
        self.cn.autocommit = True
        return self.cn

    # ---- schema discovery ----
    def get_table_schema(self, table: str) -> List[Tuple[str,str,bool]]:
        if table in self._schema_cache:
            return self._schema_cache[table]
        cn = self.connect(); cur = cn.cursor()
        cur.execute("SELECT SCHEMA_NAME(schema_id), name FROM sys.objects WHERE object_id = OBJECT_ID(?)", table)
        row = cur.fetchone()
        if not row:
            raise ValueError(f"Table not found: {table}")
        schema_name, table_name = row[0], row[1]
        cur.execute("""
            SELECT c.name AS ColumnName,
                   t.name AS DataType,
                   CASE WHEN ic.object_id IS NULL THEN 0 ELSE 1 END AS IsIdentity
            FROM sys.columns c
            JOIN sys.types t ON c.user_type_id = t.user_type_id
            LEFT JOIN sys.identity_columns ic
              ON ic.object_id = c.object_id AND ic.column_id = c.column_id
            WHERE c.object_id = OBJECT_ID(?)
            ORDER BY c.column_id
        """, f"{schema_name}.{table_name}")
        cols: List[Tuple[str,str,bool]] = [(r[0], r[1], bool(r[2])) for r in cur.fetchall()]
        cur.close()
        self._schema_cache[table] = cols
        self._id_cache[table] = {c for c,_,is_id in cols if is_id}
        logger.info("Schema %s: %s", table, [(c,t,"ID" if i else "") for c,t,i in cols])
        return cols

    def identity_columns(self, table: str) -> set[str]:
        self.get_table_schema(table)
        return self._id_cache.get(table, set())

    def coerce_rows_to_table_schema(self, table: str, rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        schema = [(c,t) for c,t,_ in self.get_table_schema(table)]
        return [TypeUtils.coerce_row_to_schema(r, schema) for r in rows]

    # ---- generic MERGE (dynamic) ----
    def merge_rows(self, table: str, rows: List[Dict[str, Any]], key_cols: Tuple[str, ...], update_when_matched: bool = True):
        if not rows: return
        schema = self.get_table_schema(table)
        id_cols = self.identity_columns(table)
        all_cols = [c for c,_,_ in schema]
        cols = [c for c in all_cols if c not in id_cols]  # exclude identities from source
        if not set(key_cols).issubset(set(all_cols)):
            raise ValueError(f"Key columns {key_cols} must exist in table {table}")

        def _norm(v): return None if v is None else str(v).strip()
        dedup: Dict[Tuple, Dict[str, Any]] = {}
        datetime_rank = [c for c in ("EventDate","SprintCompletedDate","SprintEndDate","SprintStartDate") if c in all_cols]
        for r in rows:
            key = tuple((_norm(r.get(k)) or "").casefold() for k in key_cols)
            if any(k is None for k in key):  # skip incomplete key
                continue
            current = dedup.get(key)
            if not current:
                dedup[key] = r
            else:
                def _best_ts(x):
                    for c in datetime_rank:
                        v = x.get(c)
                        if v is not None: return TypeUtils.parse_dt(v)
                    return None
                new_ts = _best_ts(r); old_ts = _best_ts(current)
                if new_ts and (not old_ts or new_ts > old_ts):
                    dedup[key] = r
        rows = list(dedup.values())
        if not rows: return

        src_cols = ", ".join(f"[{c}]" for c in cols)
        tgt_cols = src_cols
        per_row = len(cols)
        max_rows = max(1, 2000 // per_row)

        def to_params(batch):
            for r in batch:
                for c in cols:
                    yield r.get(c)

        assignments = ", ".join([f"T.[{c}] = S.[{c}]" for c in cols if c not in key_cols and c not in id_cols]) if update_when_matched else None

        def on_expr(L: str, R: str) -> str:
            return " AND ".join(
                f"LTRIM(RTRIM({L}.[{k}])) COLLATE DATABASE_DEFAULT = LTRIM(RTRIM({R}.[{k}])) COLLATE DATABASE_DEFAULT"
                for k in key_cols
            )

        i, n = 0, len(rows)
        cur = self.connect().cursor()
        while i < n:
            batch = rows[i:i+max_rows]; i += len(batch)
            row_ph = "(" + ", ".join(["?"] * per_row) + ")"
            values_clause = ", ".join([row_ph] * len(batch))
            order_by = ", ".join([f"V.[{c}] DESC" for c in ("EventDate","SprintCompletedDate","SprintEndDate","SprintStartDate") if c in cols]) or "(SELECT 0)"
            merge_sql = f"""
MERGE {table} WITH (HOLDLOCK, UPDLOCK) AS T
USING (
    SELECT *
    FROM (
        SELECT V.*,
               ROW_NUMBER() OVER (
                 PARTITION BY {", ".join([f"LTRIM(RTRIM(V.[{k}])) COLLATE DATABASE_DEFAULT" for k in key_cols])}
                 ORDER BY {order_by}
               ) AS rn
        FROM (VALUES {values_clause}) AS V ({src_cols})
    ) D
    WHERE D.rn = 1
) AS S
ON {on_expr("T","S")}
{"WHEN MATCHED THEN UPDATE SET " + assignments if assignments else ""}
WHEN NOT MATCHED BY TARGET THEN INSERT ({tgt_cols}) VALUES ({src_cols});
"""
            cur.execute(merge_sql, list(to_params(batch)))
        cur.close()



        invike

        import logging
from typing import Any, Dict, List
from .confmanager import AzureSQLConfig, SplunkConfig, TableNames
from .splunkdata import SplunkSource
from .transformation import Transformer
from .jirasprintchangemgr import DataQualityPipeline
from .rules import SprintRules, IssueRules, AssignRules
from .loader import Database

class JiraSplunkETL:
    def __init__(self,
                 splunk_cfg: SplunkConfig | None = None,
                 sql_cfg: AzureSQLConfig | None = None,
                 tables: TableNames | None = None,
                 logger: logging.Logger | None = None):
        self.logger = logger or logging.getLogger(__name__)
        self.spl = SplunkSource(splunk_cfg or SplunkConfig())
        self.db  = Database(sql_cfg or AzureSQLConfig(), tables or TableNames())
        self.tx  = Transformer()
        self.dq  = DataQualityPipeline(
            db=self.db,
            sprint_rules=SprintRules().get_rules(),
            issue_rules=IssueRules().get_rules(),
            assign_rules=AssignRules().get_rules(),
            sprint_cross=SprintRules.crosscheck,
            logger_=self.logger,
        )

    def run(self, override_query: str | None = None):
        raw = self.spl.read_jira_sprint_events(override_query)
        sprint_rows: List[Dict[str, Any]] = []
        issue_rows:  List[Dict[str, Any]] = []
        assign_rows: List[Dict[str, Any]] = []

        for rec in raw:
            payload = self.tx.parse_raw_json(rec)
            if not payload:
                continue
            sprint_rows.extend(self.tx.explode_sprints(payload, rec))
            issue_rows.append(self.tx.build_issue_row(payload))
            assign_rows.extend(self.tx.explode_assignments_from_meta_time(payload, rec))

        return self.dq.run_and_load(sprint_rows, issue_rows, assign_rows)



        
