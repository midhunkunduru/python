#!/usr/bin/env python3
"""
Pull GitHub custom property data from Splunk and load to Azure SQL.

- Streams Splunk results (export/oneshot) into Pandas DataFrames in chunks
- Creates table if not exists
- Upserts (optional) or inserts (default) with a simple dedupe key
- Maintains a watermark file to support incremental loads

Author: you
"""

import os
import json
import sys
import math
import time
import pathlib
import argparse
from datetime import datetime, timezone
from typing import Iterable, Dict, Any, List, Optional

import pandas as pd
from dotenv import load_dotenv

# Splunk SDK
import splunklib.client as splunk_client
import splunklib.results as splunk_results

# Azure SQL via SQLAlchemy + pyodbc
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

WATERMARK_FILE = ".splunk_github_props_watermark.txt"

STANDARD_COLUMNS = [
    "event_time",        # datetime (UTC)
    "repo",              # nvarchar
    "property_name",     # nvarchar
    "property_value",    # nvarchar
    "user",              # nvarchar
    "org",               # nvarchar
]

CREATE_TABLE_SQL = """
IF NOT EXISTS (
    SELECT 1
    FROM sys.tables t
    JOIN sys.schemas s ON t.schema_id = s.schema_id
    WHERE t.name = :table_name AND s.name = :schema_name
)
BEGIN
    EXEC('CREATE TABLE [' + :schema_name + '].[' + :table_name + '] (
        [event_time]       DATETIME2(3)       NOT NULL,
        [repo]             NVARCHAR(400)      NULL,
        [property_name]    NVARCHAR(400)      NULL,
        [property_value]   NVARCHAR(MAX)      NULL,
        [user]             NVARCHAR(400)      NULL,
        [org]              NVARCHAR(400)      NULL,
        [raw_json]         NVARCHAR(MAX)      NULL,
        CONSTRAINT [PK_' + :table_name + '] PRIMARY KEY CLUSTERED ([event_time],[repo],[property_name])
    )')
END
"""

INSERT_SQL = """
INSERT INTO [{schema}].[{table}](
    event_time, repo, property_name, property_value, [user], org, raw_json
)
VALUES (?, ?, ?, ?, ?, ?, ?)
"""

def load_env():
    load_dotenv(override=False)

def env_bool(name: str, default: bool = True) -> bool:
    v = os.getenv(name, str(default)).strip().lower()
    return v in ("1", "true", "yes", "y")

def get_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Load GitHub custom properties from Splunk to Azure SQL.")
    p.add_argument("--spl", required=True, help="SPL query to run. Example: \"search index=github ... | table ...\"")
    p.add_argument("--batch-size", type=int, default=5000, help="Rows per batch insert to Azure SQL.")
    p.add_argument("--no-watermark", action="store_true", help="Do not use watermark; run exactly the provided SPL.")
    p.add_argument("--earliest", default=os.getenv("DEFAULT_EARLIEST", "-7d"),
                   help="Used only if watermark not present, e.g., -7d, -24h, 0 (default from env).")
    p.add_argument("--latest", default="now", help="Splunk latest time bound (default: now).")
    p.add_argument("--verify-ssl", default=os.getenv("SPLUNK_VERIFY_SSL", "true"))
    return p.parse_args()

def splunk_connect() -> splunk_client.Service:
    host = os.getenv("SPLUNK_HOST")
    port = int(os.getenv("SPLUNK_PORT", "8089"))
    username = os.getenv("SPLUNK_USERNAME")
    password = os.getenv("SPLUNK_PASSWORD")
    verify_ssl = env_bool("SPLUNK_VERIFY_SSL", True)

    if not all([host, port, username, password]):
        print("Missing Splunk connection environment variables.", file=sys.stderr)
        sys.exit(2)

    service = splunk_client.connect(
        host=host,
        port=port,
        username=username,
        password=password,
        scheme="https",
        verify=verify_ssl
    )
    return service

def azure_engine() -> Engine:
    server = os.getenv("AZSQL_SERVER")
    database = os.getenv("AZSQL_DATABASE")
    username = os.getenv("AZSQL_USERNAME")
    password = os.getenv("AZSQL_PASSWORD")

    if not all([server, database, username, password]):
        print("Missing Azure SQL environment variables.", file=sys.stderr)
        sys.exit(2)

    # ODBC Driver 18 connection string via SQLAlchemy
    params = (
        f"DRIVER={{ODBC Driver 18 for SQL Server}};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"UID={username};"
        f"PWD={password};"
        f"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;"
    )
    engine = create_engine(f"mssql+pyodbc:///?odbc_connect={pd.io.sql._engine_params.quote_plus(params)}",
                           fast_executemany=True)
    return engine

def ensure_table(engine: Engine, schema: str, table: str):
    with engine.begin() as conn:
        conn.execute(
            text(CREATE_TABLE_SQL),
            {"schema_name": schema, "table_name": table}
        )

def read_watermark() -> Optional[str]:
    p = pathlib.Path(WATERMARK_FILE)
    if not p.exists():
        return None
    return p.read_text().strip() or None

def write_watermark(value: str):
    pathlib.Path(WATERMARK_FILE).write_text(value)

def build_spl_with_time_bounds(base_spl: str, earliest: str, latest: str) -> str:
    # If SPL already includes earliest/latest, we leave it as-is.
    # Otherwise we append them at the end.
    if "earliest=" in base_spl or "latest=" in base_spl:
        return base_spl
    return f"{base_spl} earliest={earliest} latest={latest}"

def normalize_row(row: Dict[str, Any]) -> Dict[str, Any]:
    """
    Map Splunk event fields to our standard columns; keep raw JSON.
    """
    # Prefer 'event_time' from SPL; else derive from '_time'
    event_time_val = row.get("event_time") or row.get("_time") or row.get("_indextime")
    # Convert to ISO datetime if numeric epoch
    if isinstance(event_time_val, (int, float)):
        event_dt = datetime.fromtimestamp(float(event_time_val), tz=timezone.utc)
    else:
        # Splunk times are often ISO8601 already; let pandas parse later if needed
        # but try to standardize
        try:
            event_dt = pd.to_datetime(event_time_val, utc=True).to_pydatetime()
        except Exception:
            event_dt = datetime.now(timezone.utc)

    out = {
        "event_time": event_dt.replace(tzinfo=None),  # store without tz in DATETIME2
        "repo": row.get("repo"),
        "property_name": row.get("property_name"),
        "property_value": row.get("property_value"),
        "user": row.get("user"),
        "org": row.get("org"),
        "raw_json": json.dumps(row, ensure_ascii=False),
    }
    return out

def stream_splunk_results(service: splunk_client.Service, query: str) -> Iterable[Dict[str, Any]]:
    """
    Use a blocking search and stream the results via Splunk SDK.
    """
    kwargs_search = {
        "exec_mode": "blocking",
        "search_mode": "normal",
        "count": 0,  # let Splunk page internally; SDK will iterate
    }

    if not query.strip().startswith(("search", "|")):
        query = "search " + query

    job = service.jobs.create(query, **kwargs_search)

    # Wait until done
    while not job.is_done():
        time.sleep(0.5)

    # Fetch results in a streaming manner
    for result in splunk_results.ResultsReader(job.results(output_mode="json")):
        if isinstance(result, dict):
            yield result

    job.cancel()

def df_chunks(iterable: Iterable[Dict[str, Any]], chunk_size: int) -> Iterable[pd.DataFrame]:
    buf: List[Dict[str, Any]] = []
    for row in iterable:
        buf.append(normalize_row(row))
        if len(buf) >= chunk_size:
            yield pd.DataFrame(buf)
            buf.clear()
    if buf:
        yield pd.DataFrame(buf)

def insert_batches(engine: Engine, schema: str, table: str, frames: Iterable[pd.DataFrame]) -> int:
    total = 0
    with engine.begin() as conn:
        for df in frames:
            # Ensure expected columns exist even if empty frame
            for col in ["event_time","repo","property_name","property_value","user","org","raw_json"]:
                if col not in df.columns:
                    df[col] = None

            # Use executemany for speed
            params = list(
                df[["event_time","repo","property_name","property_value","user","org","raw_json"]]
                .itertuples(index=False, name=None)
            )
            if params:
                conn.exec_driver_sql(
                    INSERT_SQL.format(schema=schema, table=table),
                    params
                )
                total += len(params)
    return total

def main():
    load_env()
    args = get_args()

    # Build time-bounded SPL if using watermark
    base_spl = args.spl
    if args.no_watermark:
        effective_spl = base_spl
        watermark_used = None
    else:
        last_wm = read_watermark()
        earliest = last_wm if last_wm else args.earliest
        latest = args.latest
        effective_spl = build_spl_with_time_bounds(base_spl, earliest, latest)
        watermark_used = earliest

    print(f"\nRunning SPL:\n{effective_spl}\n", flush=True)

    service = splunk_connect()
    engine = azure_engine()

    schema = os.getenv("AZSQL_SCHEMA", "dbo")
    table = os.getenv("AZSQL_TABLE", "GithubCustomProperties")

    ensure_table(engine, schema, table)

    # Run + stream
    rows_iter = stream_splunk_results(service, effective_spl)
    written = insert_batches(engine, schema, table, df_chunks(rows_iter, args.batch_size))

    print(f"Inserted rows: {written}")

    # Advance watermark to the latest successful bound (now)
    if not args.no_watermark:
        # Splunk "latest" we used could be "now"; we persist an absolute time for next run
        now_utc = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S")
        write_watermark(now_utc)
        print(f"Watermark updated to: {now_utc}")

if __name__ == "__main__":
    main()



search index=github sourcetype=github:custom_properties property_name=* 
| eval event_time=_time 
| table event_time repo property_name property_value user org



# Splunk
SPLUNK_HOST=your-splunk.company.com
SPLUNK_PORT=8089
SPLUNK_USERNAME=your_user
SPLUNK_PASSWORD=your_password
SPLUNK_VERIFY_SSL=true   # set to false only if you must

# Azure SQL
AZSQL_SERVER=tcp:your-sql-server.database.windows.net,1433
AZSQL_DATABASE=your_database
AZSQL_USERNAME=your_sql_login@your-sql-server
AZSQL_PASSWORD=your_password

# Target table/schema
AZSQL_SCHEMA=dbo
AZSQL_TABLE=GithubCustomProperties

# Optional: default earliest time window if no watermark exists
DEFAULT_EARLIEST=-7d
