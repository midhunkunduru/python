settings.txt

# settings.py
from dataclasses import dataclass
import os

@dataclass
class AzureSQLConfig:
    server: str = os.getenv("AZSQL_SERVER", "your-sql-server.database.windows.net")
    database: str = os.getenv("AZSQL_DATABASE", "your_db")
    username: str = os.getenv("AZSQL_USERNAME", "your_user")
    password: str = os.getenv("AZSQL_PASSWORD", "your_password")
    driver: str = os.getenv("AZSQL_DRIVER", "{ODBC Driver 18 for SQL Server}")
    encrypt: str = os.getenv("AZSQL_ENCRYPT", "yes")
    trust: str = os.getenv("AZSQL_TRUST", "no")  # TrustServerCertificate

@dataclass
class SplunkConfig:
    host: str = os.getenv("SPLUNK_HOST", "hello.splunkcloud.com")
    port: int = int(os.getenv("SPLUNK_PORT", "8089"))  # mgmt port
    scheme: str = os.getenv("SPLUNK_SCHEME", "https")
    username: str = os.getenv("SPLUNK_USERNAME", "hello")
    password: str = os.getenv("SPLUNK_PASSWORD", "hellotest@123")
    # Default SPL (override via CLI if you like)
    query: str = os.getenv(
        "SPLUNK_QUERY",
        'search index="test_developer_dev" TriggerBy="SprintChanged"'
    )
    earliest_time: str = os.getenv("SPLUNK_EARLIEST", "-24h")
    latest_time: str = os.getenv("SPLUNK_LATEST", "now")

transformer.txt

# transformer.py
import json
from datetime import datetime
from typing import Any, Dict, Optional

def _get(d: Dict[str, Any], path: str, default=None):
    cur = d
    for key in path.split("."):
        if not isinstance(cur, dict) or key not in cur:
            return default
        cur = cur[key]
    return cur

def _to_bool(v) -> Optional[bool]:
    if isinstance(v, bool): return v
    if v is None: return None
    s = str(v).strip().lower()
    if s in ("true","t","1","yes","y"): return True
    if s in ("false","f","0","no","n"): return False
    return None

def _to_dt(v) -> Optional[datetime]:
    if not v: return None
    try:
        return datetime.strptime(str(v).replace("GMT","UTC"), "%a %b %d %H:%M:%S %Z %Y")
    except Exception:
        pass
    try:
        return datetime.fromisoformat(str(v).replace("Z","+00:00"))
    except Exception:
        return None

def parse_raw_json(row: Dict[str, Any]) -> Dict[str, Any]:
    """row is a Splunk JSONResultsReader dict; _raw contains a JSON string."""
    payload = {}
    if isinstance(row, dict) and "_raw" in row:
        try:
            payload = json.loads(row["_raw"])
        except json.JSONDecodeError:
            return {}
    return payload

def build_sprint_row(payload: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "ProjectName":         _get(payload, "project.name"),
        "SprintName":          _get(payload, "sprint.name"),
        "SprintBoardId":       _get(payload, "sprint.originBoardId"),
        "SprintStartDate":     _to_dt(_get(payload, "sprint.startDate")),
        "SprintEndDate":       _to_dt(_get(payload, "sprint.endDate")),
        "SprintIsStarted":     _to_bool(_get(payload, "sprint.isStarted")),
        "SprintIsClosed":      _to_bool(_get(payload, "sprint.isClosed")),
        "SprintCompletedDate": _to_dt(_get(payload, "sprint.completeDate")),
    }

def build_issue_row(payload: Dict[str, Any]) -> Dict[str, Any]:
    sp = _get(payload, "issue.Story Points") or _get(payload, "issue.storyPoints")
    try:
        sp = float(sp) if sp not in (None, "", "null") else None
    except Exception:
        sp = None

    return {
        "Issue":               _get(payload, "issue.key"),
        "ProjectName":         _get(payload, "project.name"),
        "IssueSummary":        _get(payload, "issue.summary"),
        "IssueTypeName":       _get(payload, "issue.issueType.name"),
        "IssueAssigneeName":   _get(payload, "issue.assignee.displayName"),
        "IssueParentKey":      _get(payload, "issue.parent.key"),
        "IssueParentSummary":  _get(payload, "issue.parent.summary"),
        "IssueStatusName":     _get(payload, "issue.status.name"),
        "IssueStoryPoints":    sp,
    }

def build_assignment_row(payload: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "Issue":        _get(payload, "issue.key"),
        "ProjectName":  _get(payload, "project.name"),
        "SprintName":   _get(payload, "sprint.name"),
        "EventDate":    _to_dt(meta.get("_time")) or _to_dt(_get(payload, "eventDate")),
    }

db.txt

# db.py
from typing import Any, Dict, Iterable, List, Tuple
import pyodbc
from settings import AzureSQLConfig

def _batched(iterable: Iterable[Any], size=500):
    batch = []
    for x in iterable:
        batch.append(x)
        if len(batch) >= size:
            yield batch
            batch = []
    if batch:
        yield batch

class Database:
    # Tables and schemas
    TABLE_SPRINT = "dbo.jira_sprint"
    SPRINT_COLS: List[Tuple[str,str]] = [
        ("ProjectName",         "NVARCHAR(400) NOT NULL"),
        ("SprintName",          "NVARCHAR(400) NOT NULL"),
        ("SprintBoardId",       "NVARCHAR(200) NULL"),
        ("SprintStartDate",     "DATETIME2(3) NULL"),
        ("SprintEndDate",       "DATETIME2(3) NULL"),
        ("SprintIsStarted",     "BIT NULL"),
        ("SprintIsClosed",      "BIT NULL"),
        ("SprintCompletedDate", "DATETIME2(3) NULL"),
    ]
    SPRINT_PK = "CONSTRAINT PK_jira_sprint PRIMARY KEY (ProjectName, SprintName)"

    TABLE_ISSUE = "dbo.jira_issue"
    ISSUE_COLS: List[Tuple[str,str]] = [
        ("Issue",               "NVARCHAR(200) NOT NULL"),
        ("ProjectName",         "NVARCHAR(400) NULL"),
        ("IssueSummary",        "NVARCHAR(4000) NULL"),
        ("IssueTypeName",       "NVARCHAR(200) NULL"),
        ("IssueAssigneeName",   "NVARCHAR(400) NULL"),
        ("IssueParentKey",      "NVARCHAR(200) NULL"),
        ("IssueParentSummary",  "NVARCHAR(4000) NULL"),
        ("IssueStatusName",     "NVARCHAR(200) NULL"),
        ("IssueStoryPoints",    "FLOAT NULL"),
    ]
    ISSUE_PK = "CONSTRAINT PK_jira_issue PRIMARY KEY (Issue)"

    TABLE_ASSIGN = "dbo.jira_sprint_issue_assigned"
    ASSIGN_COLS: List[Tuple[str,str]] = [
        ("Issue",        "NVARCHAR(200) NOT NULL"),
        ("ProjectName",  "NVARCHAR(400) NOT NULL"),
        ("SprintName",   "NVARCHAR(400) NOT NULL"),
        ("EventDate",    "DATETIME2(3) NOT NULL"),
    ]
    ASSIGN_PK = "CONSTRAINT PK_jira_sprint_issue_assigned PRIMARY KEY (Issue, ProjectName, SprintName, EventDate)"

    def __init__(self, cfg: AzureSQLConfig):
        self.cfg = cfg
        self.cn: pyodbc.Connection | None = None

    def connect(self):
        if self.cn:
            return self.cn
        self.cn = pyodbc.connect(
            f"DRIVER={self.cfg.driver};"
            f"SERVER={self.cfg.server};"
            f"DATABASE={self.cfg.database};"
            f"UID={self.cfg.username};"
            f"PWD={self.cfg.password};"
            f"Encrypt={self.cfg.encrypt};"
            f"TrustServerCertificate={self.cfg.trust};"
        )
        self.cn.autocommit = True
        return self.cn

    # ---------- schema ----------
    def _create_table_if_not_exists(self, table: str,
                                    columns: List[Tuple[str,str]],
                                    pk_sql: str):
        cn = self.connect()
        cur = cn.cursor()
        cols_sql = ", ".join([f"[{c}] {t}" for c, t in columns] + [pk_sql])
        cur.execute(f"""
        IF NOT EXISTS (
            SELECT 1
            FROM sys.tables t
            JOIN sys.schemas s ON s.schema_id = t.schema_id
            WHERE t.name = PARSENAME('{table}', 1)
              AND s.name = ISNULL(PARSENAME('{table}', 2), 'dbo')
        )
        BEGIN
            DECLARE @schema sysname = ISNULL(PARSENAME('{table}', 2), 'dbo');
            DECLARE @name   sysname = PARSENAME('{table}', 1);
            DECLARE @sql nvarchar(max) =
                N'CREATE TABLE [' + @schema + N'].[' + @name + N']({cols});';
            SET @sql = REPLACE(@sql, '{cols}', N'{cols_sql}');
            EXEC sp_executesql @sql;
        END
        """.replace("{cols_sql}", cols_sql))
        cur.close()

    # ---------- upserts ----------
    def merge_sprint_rows(self, rows: List[Dict[str, Any]]):
        if not rows:
            return
        self._create_table_if_not_exists(self.TABLE_SPRINT, self.SPRINT_COLS, self.SPRINT_PK)
        cn = self.connect(); cur = cn.cursor()
        cols = [c for c, _ in self.SPRINT_COLS]
        src_cols = ", ".join(f"[{c}]" for c in cols)
        tgt_cols = src_cols
        assignments = ", ".join([f"T.[{c}] = S.[{c}]" for c in cols if c not in ("ProjectName","SprintName")])

        cur.execute(f"DECLARE @S TABLE ({src_cols.replace('[','').replace(']','')});")
        ins = f"INSERT INTO @S ({src_cols}) VALUES ({', '.join(['?']*len(cols))})"

        for b in _batched(rows, 500):
            cur.fast_executemany = True
            cur.executemany(ins, [tuple(r.get(c) for c in cols) for r in b])
            cur.execute(f"""
            MERGE {self.TABLE_SPRINT} AS T
            USING @S AS S
            ON T.ProjectName = S.ProjectName AND T.SprintName = S.SprintName
            WHEN MATCHED THEN UPDATE SET {assignments}
            WHEN NOT MATCHED THEN INSERT ({tgt_cols}) VALUES ({src_cols});
            DELETE FROM @S;
            """)
        cur.close()

    def merge_issue_rows(self, rows: List[Dict[str, Any]]):
        if not rows:
            return
        self._create_table_if_not_exists(self.TABLE_ISSUE, self.ISSUE_COLS, self.ISSUE_PK)
        cn = self.connect(); cur = cn.cursor()
        cols = [c for c, _ in self.ISSUE_COLS]
        src_cols = ", ".join(f"[{c}]" for c in cols)
        tgt_cols = src_cols
        assignments = ", ".join([f"T.[{c}] = S.[{c}]" for c in cols if c != "Issue"])

        cur.execute(f"DECLARE @S2 TABLE ({src_cols.replace('[','').replace(']','')});")
        ins = f"INSERT INTO @S2 ({src_cols}) VALUES ({', '.join(['?']*len(cols))})"

        for b in _batched(rows, 500):
            cur.fast_executemany = True
            cur.executemany(ins, [tuple(r.get(c) for c in cols) for r in b])
            cur.execute(f"""
            MERGE {self.TABLE_ISSUE} AS T
            USING @S2 AS S
            ON T.Issue = S.Issue
            WHEN MATCHED THEN UPDATE SET {assignments}
            WHEN NOT MATCHED THEN INSERT ({tgt_cols}) VALUES ({src_cols});
            DELETE FROM @S2;
            """)
        cur.close()

    def insert_assign_rows(self, rows: List[Dict[str, Any]]):
        if not rows:
            return
        self._create_table_if_not_exists(self.TABLE_ASSIGN, self.ASSIGN_COLS, self.ASSIGN_PK)
        cn = self.connect(); cur = cn.cursor()
        cols = [c for c, _ in self.ASSIGN_COLS]
        col_list = ", ".join(f"[{c}]" for c in cols)
        placeholders = ", ".join(["?"] * len(cols))
        sql = f"INSERT INTO {self.TABLE_ASSIGN} ({col_list}) VALUES ({placeholders})"
        cur.fast_executemany = True
        cur.executemany(sql, [tuple(r.get(c) for c in cols) for r in rows])
        cur.close()

etl.txt

# etl.py
from typing import Any, Dict, List
from settings import AzureSQLConfig, SplunkConfig
from splunk_source import SplunkSource
from transformer import (
    parse_raw_json, build_sprint_row, build_issue_row, build_assignment_row
)
from db import Database

class JiraSplunkETL:
    def __init__(self,
                 splunk_cfg: SplunkConfig,
                 sql_cfg: AzureSQLConfig):
        self.spl = SplunkSource(splunk_cfg)
        self.db = Database(sql_cfg)

    def run(self, override_query: str | None = None):
        raw_rows = self.spl.read_jira_sprint_events(override_query)

        sprint_rows: List[Dict[str, Any]] = []
        issue_rows:  List[Dict[str, Any]] = []
        assign_rows: List[Dict[str, Any]] = []

        for r in raw_rows:
            payload = parse_raw_json(r)
            if not payload:
                continue

            s = build_sprint_row(payload)
            i = build_issue_row(payload)
            a = build_assignment_row(payload, r)

            if s.get("ProjectName") and s.get("SprintName"):
                sprint_rows.append(s)
            if i.get("Issue"):
                issue_rows.append(i)
            if a.get("Issue") and a.get("ProjectName") and a.get("SprintName") and a.get("EventDate"):
                assign_rows.append(a)

        # Upsert/insert into Azure SQL
        self.db.merge_sprint_rows(sprint_rows)
        self.db.merge_issue_rows(issue_rows)
        self.db.insert_assign_rows(assign_rows)

        return {
            "sprints_upserted": len(sprint_rows),
            "issues_upserted": len(issue_rows),
            "assignments_inserted": len(assign_rows),
        }


hello.txt

# main.py
import argparse
from settings import AzureSQLConfig, SplunkConfig
from etl import JiraSplunkETL

def parse_args():
    ap = argparse.ArgumentParser(description="Ingest Jira Sprint events from Splunk to Azure SQL")
    ap.add_argument("--query", help="Override Splunk SPL query", default=None)
    return ap.parse_args()

def main():
    args = parse_args()
    etl = JiraSplunkETL(SplunkConfig(), AzureSQLConfig())
    stats = etl.run(override_query=args.query)
    print(f"Done: {stats}")

if __name__ == "__main__":
    main()

req.txt

splunk-sdk==2.0.2
pyodbc>=5

env.txt

export AZSQL_SERVER="your-sql-server.database.windows.net"
export AZSQL_DATABASE="your_db"
export AZSQL_USERNAME="your_user"
export AZSQL_PASSWORD="your_password"

export SPLUNK_HOST="hello.splunkcloud.com"
export SPLUNK_PORT=8089
export SPLUNK_USERNAME="hello"
export SPLUNK_PASSWORD="hellotest@123"
export SPLUNK_QUERY='search index="test_developer_dev" TriggerBy="SprintChanged"'

dependenciez

pip install -r requirements.txt
python -m jira_splunk_etl.main
# or override SPL on the fly:
python -m jira_splunk_etl.main --query 'search index=_internal | head 5'
