# -*- coding: utf-8 -*-
"""
Ingest Jira Sprint events from Splunk into Azure SQL:
  1) dbo.jira_sprint
  2) dbo.jira_issue
  3) dbo.jira_sprint_issue_assigned
"""

import json
from datetime import datetime
import pyodbc
from typing import Any, Dict, Iterable, List, Tuple

# --------- 1) SPLUNK READER (you already have this) ---------------------------
# from your_module import SplunkSource
# spl = SplunkSource()

# For illustration here, we assume:
class SplunkSource:
    def readJiraSprintEvents(self) -> List[Dict[str, Any]]:
        # Replace with your real implementation
        raise NotImplementedError


# --------- 2) AZURE SQL CONNECTION SETTINGS ----------------------------------
AZURE_SQL = {
    "server":   "your-sql-server.database.windows.net",
    "database": "your_db",
    "username": "your_user",
    "password": "your_password",
    "driver":   "{ODBC Driver 18 for SQL Server}",
    "encrypt":  "yes",
    "trust":    "no",   # TrustServerCertificate
}

# --------- 3) SCHEMA (based on your screenshots) -----------------------------

# TABLE 1: Sprint object
SQL_TABLE_SPRINT = "dbo.jira_sprint"
SPRINT_COLS = [
    # PK: SprintName + ProjectName (safer than SprintName alone)
    ("ProjectName",         "NVARCHAR(400) NOT NULL"),
    ("SprintName",          "NVARCHAR(400) NOT NULL"),
    ("SprintBoardId",       "NVARCHAR(200) NULL"),
    ("SprintStartDate",     "DATETIME2(3) NULL"),
    ("SprintEndDate",       "DATETIME2(3) NULL"),
    ("SprintIsStarted",     "BIT NULL"),
    ("SprintIsClosed",      "BIT NULL"),
    ("SprintCompletedDate", "DATETIME2(3) NULL"),
]
SPRINT_PK = "CONSTRAINT PK_jira_sprint PRIMARY KEY (ProjectName, SprintName)"

# TABLE 2: Issue object
SQL_TABLE_ISSUE = "dbo.jira_issue"
ISSUE_COLS = [
    # PK: Issue
    ("Issue",               "NVARCHAR(200) NOT NULL"),
    ("ProjectName",         "NVARCHAR(400) NULL"),
    ("IssueSummary",        "NVARCHAR(4000) NULL"),
    ("IssueTypeName",       "NVARCHAR(200) NULL"),
    ("IssueAssigneeName",   "NVARCHAR(400) NULL"),
    ("IssueParentKey",      "NVARCHAR(200) NULL"),
    ("IssueParentSummary",  "NVARCHAR(4000) NULL"),
    ("IssueStatusName",     "NVARCHAR(200) NULL"),
    ("IssueStoryPoints",    "FLOAT NULL"),
]
ISSUE_PK = "CONSTRAINT PK_jira_issue PRIMARY KEY (Issue)"

# TABLE 3: Assignment link (event of issue assigned to sprint)
SQL_TABLE_ASSIGN = "dbo.jira_sprint_issue_assigned"
ASSIGN_COLS = [
    # Composite PK so repeated events with different timestamps can be stored
    ("Issue",        "NVARCHAR(200) NOT NULL"),
    ("ProjectName",  "NVARCHAR(400) NOT NULL"),
    ("SprintName",   "NVARCHAR(400) NOT NULL"),
    ("EventDate",    "DATETIME2(3) NOT NULL"),
]
ASSIGN_PK = "CONSTRAINT PK_jira_sprint_issue_assigned PRIMARY KEY (Issue, ProjectName, SprintName, EventDate)"

# --------- 4) HELPERS --------------------------------------------------------

def connect_sql() -> pyodbc.Connection:
    cn = pyodbc.connect(
        f"DRIVER={AZURE_SQL['driver']};"
        f"SERVER={AZURE_SQL['server']};"
        f"DATABASE={AZURE_SQL['database']};"
        f"UID={AZURE_SQL['username']};"
        f"PWD={AZURE_SQL['password']};"
        f"Encrypt={AZURE_SQL['encrypt']};"
        f"TrustServerCertificate={AZURE_SQL['trust']};"
    )
    cn.autocommit = True
    return cn

def create_table_if_not_exists(
    cursor: pyodbc.Cursor,
    table: str,
    columns: List[Tuple[str, str]],
    pk_sql: str
) -> None:
    cols_sql = ", ".join([f"[{c}] {t}" for c, t in columns] + [pk_sql])
    cursor.execute(f"""
    IF NOT EXISTS (
        SELECT 1
        FROM sys.tables t
        JOIN sys.schemas s ON s.schema_id = t.schema_id
        WHERE t.name = PARSENAME('{table}', 1)
          AND s.name = ISNULL(PARSENAME('{table}', 2), 'dbo')
    )
    BEGIN
        DECLARE @schema sysname = ISNULL(PARSENAME('{table}', 2), 'dbo');
        DECLARE @name   sysname = PARSENAME('{table}', 1);
        DECLARE @sql nvarchar(max) =
            N'CREATE TABLE [' + @schema + N'].[' + @name + N']({cols});';
        SET @sql = REPLACE(@sql, '{cols}', N'{cols_sql}');
        EXEC sp_executesql @sql;
    END
    """.replace("{cols_sql}", cols_sql))

def get(d: Dict[str, Any], path: str, default=None):
    """
    Safe nested getter using dot paths like 'project.name' or 'issue.assignee.displayName'.
    Also supports spaces in keys e.g. 'issue.Story Points'.
    """
    cur = d
    for key in path.split("."):
        if not isinstance(cur, dict) or key not in cur:
            return default
        cur = cur[key]
    return cur

def to_bool(v):
    if isinstance(v, bool): return v
    if v is None: return None
    s = str(v).strip().lower()
    if s in ("true", "t", "1", "yes", "y"): return True
    if s in ("false", "f", "0", "no", "n"): return False
    return None

def to_dt(v):
    if not v: return None
    # Try a few formats (your screenshot shows "Mon Jul 21 05:00:00 GMT 2025")
    try:
        return datetime.strptime(str(v).replace("GMT", "UTC"), "%a %b %d %H:%M:%S %Z %Y")
    except Exception:
        pass
    try:
        return datetime.fromisoformat(str(v).replace("Z", "+00:00"))
    except Exception:
        return None

def batched(iterable: Iterable[Any], size=500):
    batch = []
    for x in iterable:
        batch.append(x)
        if len(batch) >= size:
            yield batch
            batch = []
    if batch:
        yield batch

# --------- 5) TRANSFORMS (exactly as in your mapping) ------------------------

def build_sprint_row(payload: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "ProjectName":         get(payload, "project.name"),
        "SprintName":          get(payload, "sprint.name"),
        "SprintBoardId":       get(payload, "sprint.originBoardId"),
        "SprintStartDate":     to_dt(get(payload, "sprint.startDate")),
        "SprintEndDate":       to_dt(get(payload, "sprint.endDate")),
        "SprintIsStarted":     to_bool(get(payload, "sprint.isStarted")),
        "SprintIsClosed":      to_bool(get(payload, "sprint.isClosed")),
        "SprintCompletedDate": to_dt(get(payload, "sprint.completeDate")),
    }

def build_issue_row(payload: Dict[str, Any]) -> Dict[str, Any]:
    # Story points field sometimes appears as 'Story Points' (with a space)
    sp = get(payload, "issue.Story Points")
    if sp in (None, ""):
        sp = get(payload, "issue.storyPoints")
    try:
        sp = float(sp) if sp not in (None, "", "null") else None
    except Exception:
        sp = None

    return {
        "Issue":               get(payload, "issue.key"),
        "ProjectName":         get(payload, "project.name"),
        "IssueSummary":        get(payload, "issue.summary"),
        "IssueTypeName":       get(payload, "issue.issueType.name"),
        "IssueAssigneeName":   get(payload, "issue.assignee.displayName"),
        "IssueParentKey":      get(payload, "issue.parent.key"),
        "IssueParentSummary":  get(payload, "issue.parent.summary"),
        "IssueStatusName":     get(payload, "issue.status.name"),
        "IssueStoryPoints":    sp,
    }

def build_assignment_row(payload: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "Issue":        get(payload, "issue.key"),
        "ProjectName":  get(payload, "project.name"),
        "SprintName":   get(payload, "sprint.name"),
        "EventDate":    to_dt(meta.get("_time")) or to_dt(get(payload, "eventDate")),
    }

# --------- 6) UPSERTS (MERGE) ------------------------------------------------

def merge_sprint_rows(cur: pyodbc.Cursor, rows: List[Dict[str, Any]]):
    if not rows: return
    create_table_if_not_exists(cur, SQL_TABLE_SPRINT, SPRINT_COLS, SPRINT_PK)
    cols = [c for c, _ in SPRINT_COLS]
    src_cols = ", ".join(f"[{c}]" for c in cols)
    tgt_cols = src_cols
    assignments = ", ".join([f"T.[{c}] = S.[{c}]" for c in cols if c not in ("ProjectName", "SprintName")])

    # Use a table variable for MERGE
    cur.execute(f"DECLARE @S TABLE ({src_cols.replace('[', '').replace(']', '')});")  # schema not needed for table var
    insert_sql = f"INSERT INTO @S ({src_cols}) VALUES ({', '.join(['?']*len(cols))})"
    for b in batched(rows, 500):
        cur.fast_executemany = True
        cur.executemany(insert_sql, [tuple(r.get(c) for c in cols) for r in b])
        cur.execute(f"""
        MERGE {SQL_TABLE_SPRINT} AS T
        USING @S AS S
        ON T.ProjectName = S.ProjectName AND T.SprintName = S.SprintName
        WHEN MATCHED THEN UPDATE SET {assignments}
        WHEN NOT MATCHED THEN INSERT ({tgt_cols}) VALUES ({src_cols});
        DELETE FROM @S;  -- clear for next batch
        """)

def merge_issue_rows(cur: pyodbc.Cursor, rows: List[Dict[str, Any]]):
    if not rows: return
    create_table_if_not_exists(cur, SQL_TABLE_ISSUE, ISSUE_COLS, ISSUE_PK)
    cols = [c for c, _ in ISSUE_COLS]
    src_cols = ", ".join(f"[{c}]" for c in cols)
    tgt_cols = src_cols
    assignments = ", ".join([f"T.[{c}] = S.[{c}]" for c in cols if c != "Issue"])

    cur.execute(f"DECLARE @S2 TABLE ({src_cols.replace('[', '').replace(']', '')});")
    insert_sql = f"INSERT INTO @S2 ({src_cols}) VALUES ({', '.join(['?']*len(cols))})"
    for b in batched(rows, 500):
        cur.fast_executemany = True
        cur.executemany(insert_sql, [tuple(r.get(c) for c in cols) for r in b])
        cur.execute(f"""
        MERGE {SQL_TABLE_ISSUE} AS T
        USING @S2 AS S
        ON T.Issue = S.Issue
        WHEN MATCHED THEN UPDATE SET {assignments}
        WHEN NOT MATCHED THEN INSERT ({tgt_cols}) VALUES ({src_cols});
        DELETE FROM @S2;
        """)

def insert_assign_rows(cur: pyodbc.Cursor, rows: List[Dict[str, Any]]):
    if not rows: return
    create_table_if_not_exists(cur, SQL_TABLE_ASSIGN, ASSIGN_COLS, ASSIGN_PK)
    cols = [c for c, _ in ASSIGN_COLS]
    col_list = ", ".join(f"[{c}]" for c in cols)
    placeholders = ", ".join(["?"] * len(cols))
    sql = f"INSERT INTO {SQL_TABLE_ASSIGN} ({col_list}) VALUES ({placeholders})"
    # Because PK is composite, duplicate events will naturally fail; ignore dup errors if needed
    cur.fast_executemany = True
    try:
        cur.executemany(sql, [tuple(r.get(c) for c in cols) for r in rows])
    except pyodbc.Error as e:
        # If you prefer true upsert semantics, convert to MERGE similar to the others.
        raise

# --------- 7) MAIN ETL -------------------------------------------------------

def run():
    spl = SplunkSource()
    raw_results = spl.readJiraSprintEvents()  # [{..., "_raw": "...json..."}, ...]

    sprint_rows: List[Dict[str, Any]] = []
    issue_rows:  List[Dict[str, Any]] = []
    assign_rows: List[Dict[str, Any]] = []

    for r in raw_results:
        if not isinstance(r, dict) or "_raw" not in r:
            continue
        try:
            payload = json.loads(r["_raw"])
        except json.JSONDecodeError:
            continue

        srow = build_sprint_row(payload)
        irow = build_issue_row(payload)
        arow = build_assignment_row(payload, r)

        # basic sanity: require keys present
        if srow["ProjectName"] and srow["SprintName"]:
            sprint_rows.append(srow)
        if irow["Issue"]:
            issue_rows.append(irow)
        if arow["Issue"] and arow["ProjectName"] and arow["SprintName"] and arow["EventDate"]:
            assign_rows.append(arow)

    if not (sprint_rows or issue_rows or assign_rows):
        print("No valid rows parsed from Splunk results.")
        return

    cn = connect_sql()
    cur = cn.cursor()

    # Upsert/insert
    merge_sprint_rows(cur, sprint_rows)
    merge_issue_rows(cur, issue_rows)
    insert_assign_rows(cur, assign_rows)

    cur.close()
    cn.close()
    print(
        f"Upserted {len(sprint_rows)} sprint rows, "
        f"{len(issue_rows)} issue rows, "
        f"inserted {len(assign_rows)} sprint->issue assignments."
    )

if __name__ == "__main__":
    run()


    
